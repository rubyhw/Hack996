{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuHg7C0FBnzI"
      },
      "source": [
        "# **Use MSTR Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdjyrzT7KYBt"
      },
      "source": [
        "# **HMM**\n",
        "Dataset used include\n",
        "coinbase_premium_gap getting from CryptoQuant and OHLCV data getting from Alphavantage, hourly data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw9jn62zKlTT",
        "outputId": "bc12e31d-4c14-49d9-f968-6b8c0d17cdfd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Select one of the data sources\n",
        "# data_source = \"/content/mstr_full_market_data_with_indicators.csv\"\n",
        "# data_source = \"/content/full_market_data_with_indicators.csv\"\n",
        "# data_source =\"/content/full_market_data_with_indicators.csv\"\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(data_source, parse_dates=[\"datetime\"])\n",
        "\n",
        "df = df.sort_values(\"datetime\").reset_index(drop=True)\n",
        "\n",
        "# 1. Create cyclical time features\n",
        "df['hour_sin'] = np.sin(2 * np.pi * df['datetime'].dt.hour/24)\n",
        "df['hour_cos'] = np.cos(2 * np.pi * df['datetime'].dt.hour/24)\n",
        "\n",
        "# 2. Select all numerical features for PCA\n",
        "# Exclude datetime and any other non-numerical columns\n",
        "features = [\n",
        "    'coinbase_premium_gap',\n",
        "    'coinbase_premium_gap_usdt_adjusted',\n",
        "    'coinbase_premium_index',\n",
        "    'coinbase_premium_index_usdt_adjusted',\n",
        "    'open',\n",
        "    'high',\n",
        "    'low',\n",
        "    'close',\n",
        "    'volume',\n",
        "    'sma',\n",
        "    'rsi',\n",
        "    'hour_sin',\n",
        "    'hour_cos'  # our transformed time features\n",
        "]\n",
        "\n",
        "# 3. Standardize the features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(df[features])\n",
        "\n",
        "# Impute NaN values with the mean of each column\n",
        "imputer = SimpleImputer(strategy='mean') # Create an imputer instance\n",
        "scaled_features = imputer.fit_transform(scaled_features) # Impute NaN values\n",
        "\n",
        "# 4. Perform PCA\n",
        "# Choose number of components (can adjust this)\n",
        "n_components = min(4, len(features))\n",
        "pca = PCA(n_components=n_components)\n",
        "principal_components = pca.fit_transform(scaled_features)\n",
        "\n",
        "# 5. Create DataFrame with the principal components\n",
        "pca_df = pd.DataFrame(\n",
        "    data=principal_components,\n",
        "    columns=[f'PC{i+1}' for i in range(n_components)]\n",
        ")\n",
        "\n",
        "def select_top_features_via_pca(df, features, n_components=4, top_n=6):\n",
        "    scaler = StandardScaler()\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    scaled = scaler.fit_transform(df[features])\n",
        "    scaled = imputer.fit_transform(scaled)\n",
        "\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca.fit(scaled)\n",
        "\n",
        "    loadings = np.abs(pca.components_.T)\n",
        "    feature_importance = pd.Series(loadings.sum(axis=1), index=features)\n",
        "    top_features = feature_importance.sort_values(ascending=False).head(top_n).index.tolist()\n",
        "    return df[top_features]\n",
        "\n",
        "# 6. Optionally: Combine with original data\n",
        "final_df = pd.concat([df, pca_df], axis=1)\n",
        "\n",
        "# 7. Examine explained variance\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
        "print(\"Total explained variance:\", sum(pca.explained_variance_ratio_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJrMz0JWk-f1"
      },
      "outputs": [],
      "source": [
        "loadings = pca.components_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "dMnGJ5EblBAK",
        "outputId": "daec2e65-da83-464f-b02f-51124f27106c"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a DataFrame for loadings\n",
        "loadings_df = pd.DataFrame(loadings, columns=features, index=[f'PC{i+1}' for i in range(n_components)])\n",
        "\n",
        "# Plot a heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(loadings_df, annot=True, cmap='viridis')\n",
        "plt.title('Loadings of Principal Components')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHup6mo3l-c5"
      },
      "source": [
        "# **HMM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xBlp4LZl8VV",
        "outputId": "412ef037-60ce-4fdc-d8d6-47c062d6e4db"
      },
      "outputs": [],
      "source": [
        "pip install hmmlearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF7uzi3xmOHT"
      },
      "outputs": [],
      "source": [
        "from hmmlearn import hmm\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "X_selected = select_top_features_via_pca(df, features, n_components=4, top_n=6)\n",
        "\n",
        "# Train HMM to predict market regimes\n",
        "scaler = StandardScaler()\n",
        "# hmm_features = [\"coinbase_premium_gap_usdt_adjusted\", \"coinbase_premium_index_usdt_adjusted\", \"open\", \"high\", \"low\", \"close\", \"returns\", \"volume\",\"sma\",\"hour_cos\"]\n",
        "# Assuming X_selected was originally derived from df\n",
        "common_index = df.index.intersection(X_selected.index)\n",
        "\n",
        "# Filter both DataFrames to the common index\n",
        "X_selected = X_selected.loc[common_index]\n",
        "df = df.loc[common_index]\n",
        "\n",
        "# Now apply the rest of your logic\n",
        "scaled = scaler.fit_transform(X_selected)\n",
        "model_hmm = hmm.GaussianHMM(n_components=3, covariance_type=\"diag\", n_iter=1000)\n",
        "model_hmm.fit(scaled)\n",
        "df[\"hmm_state\"] = model_hmm.predict(scaled)  # Add HMM states as a feature\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdzjNrk5xHen"
      },
      "source": [
        "# **Rolling Window Selection Before LSTM**\n",
        "LSTM Window Size Selection Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAR9CIanxOxV",
        "outputId": "3eb35230-9809-4845-a28c-21e807f7e041"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ======= Helper Functions ======= #\n",
        "# Removed features here to prevent collision\n",
        "def create_sequences(data, window_size=60):\n",
        "    x, y = [], []\n",
        "    for i in range(window_size, len(data)):\n",
        "        x.append(data[i-window_size:i, :-1])  # all except target\n",
        "        y.append(data[i, -1])  # target = close price\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "def build_lstm(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=50))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=1))  # Predict 1 output\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "def evaluate_window_size(df, features, target_col='close', window_sizes=[30, 45, 60]):\n",
        "    results = []\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(df[features])\n",
        "\n",
        "    # Append target column at the end\n",
        "    target_scaled = scaler.fit_transform(df[[target_col]])\n",
        "    scaled_data = np.hstack((scaled_data, target_scaled))\n",
        "\n",
        "    for window in window_sizes:\n",
        "        try:\n",
        "            X, y = create_sequences(scaled_data, window_size=window)\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "            model = build_lstm((X_train.shape[1], X_train.shape[2]))\n",
        "            model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "            y_pred = model.predict(X_test)\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            results.append((window, mse))\n",
        "\n",
        "            print(f\"Window Size: {window} | MSE: {mse:.6f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Window Size {window} failed: {e}\")\n",
        "\n",
        "    # Select the best window\n",
        "    best_window, best_mse = min(results, key=lambda x: x[1])\n",
        "    print(f\"\\n✅ Best window size: {best_window} (MSE: {best_mse:.6f})\")\n",
        "\n",
        "    return best_window, results\n",
        "\n",
        "# ======= Main Code ======= #\n",
        "\n",
        "df[\"returns\"] = df[\"close\"].pct_change()\n",
        "df = df.dropna()\n",
        "\n",
        "# Select features (example)\n",
        "# Fixed feature list, added comma between 'hmm_state' and next feature\n",
        "features = [\"hmm_state\",\"coinbase_premium_gap\", \"coinbase_premium_index\",\"coinbase_premium_gap_usdt_adjusted\", \"coinbase_premium_index_usdt_adjusted\",\n",
        "            \"open\", \"high\", \"low\", \"close\", \"volume\", \"rsi\", \"sma\"]\n",
        "\n",
        "# Run pipeline\n",
        "best_window, all_results = evaluate_window_size(df, features, target_col=\"close\", window_sizes=[30, 45, 60])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVehxEEf4LaB"
      },
      "source": [
        "# **Feature Selection for LSTM**\n",
        "Use XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK8K3mDq4YYY",
        "outputId": "ba168957-1a01-45b4-c507-e534543d6356"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Full feature list\n",
        "features = [\"hmm_state\", \"coinbase_premium_gap\", \"coinbase_premium_index\", \"coinbase_premium_gap_usdt_adjusted\", \"coinbase_premium_index_usdt_adjusted\",\n",
        "            \"open\", \"high\", \"low\", \"close\", \"volume\", \"rsi\", \"sma\"]\n",
        "\n",
        "target = \"close\"\n",
        "\n",
        "# Scale data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(df[features])\n",
        "scaled_target = scaler.fit_transform(df[[target]])\n",
        "\n",
        "# Split before LSTM (no sequence yet)\n",
        "X_train, X_test, y_train, y_test = train_test_split(scaled_features, scaled_target, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Step 1: Feature selection using XGBoost\n",
        "xgb = XGBRegressor(n_estimators=100)\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Select features based on importance threshold\n",
        "selector = SelectFromModel(xgb, threshold=\"median\", prefit=True)\n",
        "X_train_selected = selector.transform(X_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "\n",
        "# Get selected feature names\n",
        "selected_mask = selector.get_support()\n",
        "selected_features = [f for f, s in zip(features, selected_mask) if s]\n",
        "\n",
        "print(\"✅ Selected features:\", selected_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyYv3pc_o-u2"
      },
      "source": [
        "# **Train LSTM**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDjjKFs-pFPF"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Scaling the data using MinMaxScaler for LSTM (LSTM is sensitive to scaling)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(df[selected_features])\n",
        "\n",
        "# Create sequences (look back window)\n",
        "def create_sequences(data, best_window):\n",
        "    x = []\n",
        "    y = []\n",
        "    for i in range(best_window, len(data)):\n",
        "        x.append(data[i-best_window:i, :-1])  # Use all features except the target (close price)\n",
        "        y.append(data[i, -1])  # Predict the next close price\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "# Create sequences\n",
        "X, y = create_sequences(scaled_data, best_window)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Reshape the data for LSTM: (samples, time steps, features)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ziork2Hq0zWC"
      },
      "source": [
        "# To prevent overfitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEbErQAopQSo",
        "outputId": "cbf782c3-a58c-4d9a-a490-36ded68a98fe"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Build the LSTM model with regularization\n",
        "model = Sequential()\n",
        "\n",
        "# LSTM Layer with Dropout for regularization\n",
        "model.add(LSTM(units=100, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
        "               kernel_regularizer=regularizers.l2(0.01), recurrent_regularizer=regularizers.l2(0.01))) # L2 Regularization\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# LSTM Layer\n",
        "model.add(LSTM(units=100, return_sequences=False,\n",
        "               kernel_regularizer=regularizers.l2(0.01), recurrent_regularizer=regularizers.l2(0.01)))  # L2 Regularization\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(units=1))  # Predicting next close price\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K30gV0D_pTsX",
        "outputId": "0f419fef-b3ff-466e-f0b9-533a7762303c"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Optionally, you can plot the training history to check for overfitting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "T2xcoyq5rw4H",
        "outputId": "7328fd09-0b43-45b1-9859-1636978ad4e3"
      },
      "outputs": [],
      "source": [
        "# Predict the stock price on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Get the number of features used in the LSTM model\n",
        "num_features = X_test.shape[2]\n",
        "\n",
        "# Reshape to 2D for inverse_transform\n",
        "X_test_2D = X_test[:, -1, :].reshape(-1, num_features)\n",
        "\n",
        "# Create a dummy array with the correct number of features the scaler was fit on\n",
        "dummy_array = np.zeros((X_test_2D.shape[0], len(selected_features))) # Use len(selected_features) instead of len(features)\n",
        "\n",
        "# Fill the dummy array with the relevant features from X_test_2D\n",
        "dummy_array[:, :num_features] = X_test_2D\n",
        "\n",
        "# Now use inverse_transform on the dummy array\n",
        "X_test_rescaled = scaler.inverse_transform(dummy_array)[:, :num_features]\n",
        "\n",
        "# Rescale the predicted values and actual values by appending them to the features\n",
        "y_pred_rescaled = scaler.inverse_transform(np.hstack((X_test_rescaled, y_pred.reshape(-1, 1))))[:, -1]\n",
        "\n",
        "# Ensure y_test and y_pred have the same length for rescaling\n",
        "y_test_rescaled = scaler.inverse_transform(np.hstack((X_test_rescaled, y_test[:len(y_pred)].reshape(-1, 1))))[:, -1]\n",
        "\n",
        "# Plot the actual vs predicted values\n",
        "plt.plot(y_test_rescaled, label='Actual Prices')\n",
        "plt.plot(y_pred_rescaled, label='Predicted Prices')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0E0biw3r51v"
      },
      "source": [
        "# **Hybrid LSTM+XGBoost**\n",
        "**LSTM: Forecasts future market trends — for example, predicting future close prices, returns, or price directions.**  \n",
        "**XGBoost: Takes the LSTM prediction (and maybe other features) as input, and classifies what action to take: Buy, Sell, or Hold**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtCYZGDQr31Z"
      },
      "outputs": [],
      "source": [
        "# Construct DataFrame for XGBoost\n",
        "lstm_predicted_returns = np.append([0], np.diff(y_pred_rescaled) / y_pred_rescaled[:-1])\n",
        "\n",
        "xgb_features = pd.DataFrame({\n",
        "    \"lstm_pred_return\": lstm_predicted_returns.flatten(),  # can add other engineered features\n",
        "    \"hmm_state\": df[\"hmm_state\"].values[:len(lstm_predicted_returns.flatten())]  # Trim to length of lstm_predicted_returns\n",
        "})\n",
        "\n",
        "# Define target again (Buy/Sell/Hold)\n",
        "threshold = 0.005\n",
        "target = np.zeros_like(lstm_predicted_returns.flatten())\n",
        "target[lstm_predicted_returns.flatten() > threshold] = 1   # Buy\n",
        "target[lstm_predicted_returns.flatten() < -threshold] = -1 # Sell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udoZ8PACr9zc"
      },
      "outputs": [],
      "source": [
        "label_map = {-1: 0, 0: 1, 1: 2}\n",
        "target_mapped = pd.Series(target).map(label_map)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "fC_BAMc_sAmn",
        "outputId": "10ec42e2-ef20-4d40-c559-7a8f35f25913"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Assuming you want to keep num_class=3, adjust the target mapping\n",
        "label_map = {-1: 0, 0: 1, 1: 2}  # Ensure all classes [0, 1, 2] are present\n",
        "target_mapped = pd.Series(target).map(label_map)\n",
        "\n",
        "# Check if all classes are present in the target variable\n",
        "unique_classes = target_mapped.unique()\n",
        "expected_classes = np.arange(3)  # [0, 1, 2] for num_class=3\n",
        "\n",
        "if len(unique_classes) < 3:  # If there are fewer than 3 unique classes\n",
        "    # Handle imbalanced classes - We'll use a binary classification approach\n",
        "    # Change objective to 'binary:logistic' for binary classification\n",
        "    objective = 'binary:logistic'\n",
        "\n",
        "    # Since we only have 2 classes, change num_class to 2 or remove it\n",
        "    num_class = 2  # Or remove num_class parameter entirely\n",
        "\n",
        "    # Modify the label_map to handle the two existing classes:\n",
        "    label_map = {0: 0, 1: 1, 2: 1}  # Maps both buy(2) and hold(1) to 1\n",
        "    target_mapped = pd.Series(target).map(label_map)\n",
        "else:  # If all 3 classes are present\n",
        "    # Use multiclass classification\n",
        "    objective = 'multi:softmax'\n",
        "    num_class = 3\n",
        "\n",
        "# Ensure target_mapped and xgb_features have the same length\n",
        "target_mapped = target_mapped[:len(xgb_features)]\n",
        "\n",
        "model_xgb = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=3,\n",
        "    learning_rate=0.1,\n",
        "    objective=objective,  # Use the appropriate objective\n",
        "    # num_class=num_class,  # Use the appropriate num_class # num_class is automatically inferred for binary classification\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model_xgb.fit(xgb_features, target_mapped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvNGvMyCsCdU"
      },
      "source": [
        "# **Test Signal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "vfgug1jusE9V",
        "outputId": "fdf90355-be6b-4608-a175-f856b7e80d2a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# --- Step 1: Predict with XGBoost model ---\n",
        "predictions = model_xgb.predict(xgb_features)\n",
        "\n",
        "# --- Step 2: Remap prediction labels ---\n",
        "# You used label_map = {-1: 0, 0: 1, 1: 2}\n",
        "inverse_label_map = {0: -1, 1: 0, 2: 1}\n",
        "signals = np.vectorize(inverse_label_map.get)(predictions)\n",
        "\n",
        "# --- Step 3: Calculate strategy returns based on predicted signals ---\n",
        "# Get the actual test returns\n",
        "actual_returns = np.append([0], np.diff(y_test_rescaled) / y_test_rescaled[:-1])\n",
        "\n",
        "# Make sure lengths match\n",
        "signals = signals[:len(actual_returns)]\n",
        "\n",
        "# Shift signal so that we execute at the next timestep\n",
        "shifted_signals = np.roll(signals, 1)\n",
        "shifted_signals[0] = 0  # Neutral at the start\n",
        "\n",
        "strategy_returns = shifted_signals * actual_returns\n",
        "\n",
        "# --- Step 4: Compute metrics ---\n",
        "\n",
        "# Cumulative returns\n",
        "cumulative_returns = np.cumprod(1 + strategy_returns) - 1\n",
        "\n",
        "# Sharpe Ratio\n",
        "sharpe_ratio = np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-9) * np.sqrt(252)\n",
        "\n",
        "# Max Drawdown\n",
        "cumulative_curve = (1 + strategy_returns).cumprod()\n",
        "rolling_max = np.maximum.accumulate(cumulative_curve)\n",
        "drawdown = cumulative_curve / rolling_max - 1\n",
        "max_drawdown = drawdown.min()\n",
        "\n",
        "# Trading Frequency\n",
        "trading_actions = np.count_nonzero(shifted_signals)\n",
        "trading_frequency = trading_actions / len(shifted_signals)\n",
        "trades_per_year = trading_frequency * 252\n",
        "\n",
        "# --- Step 5: Plot ---\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(cumulative_returns, label='Strategy Cumulative Return')\n",
        "plt.title(\"XGBoost Strategy based on LSTM Predictions\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Cumulative Return\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# --- Step 6: Print Results ---\n",
        "print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
        "print(f\"Max Drawdown: {max_drawdown:.2%}\")\n",
        "print(f\"Trading Frequency: {trading_frequency:.2%} (~{trades_per_year:.1f} trades/year)\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
